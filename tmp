= External tables 
Writable external tables allow only INSERT operations.Writable external tables support:* Selecting data from database tables to insert into the writable external table.* Sending data to an application as a stream of data. For example, unload data from Greenplum Database and send it to an application that connects to another database or ETL tool to load the data elsewhere.* Receiving output from Greenplum parallel MapReduce calculations. 
Custom protocol allows you to connect Greenplum Database to a data source that cannot be accessed with predefined protocols.Creating a custom protocol requires that you implement a set of C functions with specified interfaces, declare the functions in Greenplum Database, and then use the CREATE TRUSTED PROTOCOL command to enable the protocol in the database.
== File based external tables Each file must reside on a segment host in a location accessible by the Greenplum superuser (gpadmin).The number of URIs you specify in the LOCATION clause is the number of segment instances that will work in parallel to access the external table.
CREATE EXTERNAL TABLE ext_expenses (   name text, date date, amount float4, category text, desc1 text ) LOCATION ('file://host1:5432/data/expense/*.csv',           'file://host2:5432/data/expense/*.csv',           'file://host3:5432/data/expense/*.csv') FORMAT 'CSV' (HEADER); 
One primary segment is allocated to one URI specified for a given server host. You cannot have more URI's per host than primary segments on the host. Maximum number of files permitted per external table (=number of primary segments):SELECT * FROM pg_max_external_files;
== gpfdist Uncopresses gzip and bz2 files automatically You can use the wildcard character (*) or other C-style pattern matching to denote multiple files to read.gp_external_max_segs limits maximum number of segments used in parallel for a gpfdist load (64 default)
Run gpfdist with --ssl to encrypt traffic between Greenplum and file server (gpfdists protocol)
External table queries that use LIMIT end the connection after retrieving the rows, causing an HTTP socket error. If you use LIMIT in queries of external tables that use the gpfdist:// or http:// protocols, ignore these errors – data is returned to the database as expected.
Use pipes (|) to separate formatted text when you submit files to gpfdist. Greenplum Database encloses comma-separated text strings in single or double quotes. gpfdist has to remove the quotes to parse the strings. Using pipes to separate formatted text avoids the extra step and improves performance.
Running in backgroud and logging output/error messages into a file 
nohup gpfdist -d /var/load_files -p 8081 -l /home/gpadmin/log &
gpfdist is a http server and accessibility from Greenplum segments can be tested using wget http://gpfdist_hostname:port/filename
To see information about the ports that gpfdist tests, use the -V option.
Multiple gpfdist instances # CREATE EXTERNAL TABLE ext_expenses ( name text,    date date,  amount float4, category text, desc1 text )    LOCATION ('gpfdists://etlhost-1:8081/*.txt',              'gpfdists://etlhost-2:8082/*.txt')   FORMAT 'TEXT' ( DELIMITER '|' NULL ' ') ;
Multiple files in CSV format with header rows# CREATE EXTERNAL TABLE ext_expenses ( name text,    date date,  amount float4, category text, desc1 text )    LOCATION ('file://filehost/data/international/*',              'file://filehost/data/regional/*',             'file://filehost/data/supplement/*.csv')   FORMAT 'CSV' (HEADER);
Writable External Table with gpfdistCREATE WRITABLE EXTERNAL TABLE sales_out (LIKE sales)    LOCATION ('gpfdist://etl1:8081/sales.out')   FORMAT 'TEXT' ( DELIMITER '|' NULL ' ')   DISTRIBUTED BY (txn_id);
== gphdfs 
Greenplum determines the connections between the segments and nodes.Each Greenplum segment reads one set of Hadoop data blocks.
https://gpdb.docs.pivotal.io/43160/graphics/ext_tables_hadoop.jpg
Support for Pivotal HD, Greenplum HD have been deprecated and will be removed in a future release 
GRANT INSERT ON PROTOCOL gphdfs TO gpadmin; -- needed for writable external tables 
You can specify one path for a readable external table with gphdfs. Wildcard characters are allowed. If you specify a directory, the default is all files in the directory. You can specify only a directory for writable external tables.
Readable external table for an HDFS file named filename.txt on port 8081.
# CREATE EXTERNAL TABLE ext_expenses (         name text,         date date,         amount float4,         category text,         desc1 text )    LOCATION ('gphdfs://hdfshost-1:8081/data/filename.txt')    FORMAT 'TEXT' (DELIMITER ',');
Readable external tables that have a custom format located in the same HDFS directory on port 8081.
# CREATE EXTERNAL TABLE ext_expenses    LOCATION ('gphdfs://hdfshost-1:8081/data/custdat*.dat')    FORMAT 'custom' (formatter='gphdfs_import');
HDFS directory for a writable external table on port 8081 with all compression options specified.
# CREATE WRITABLE EXTERNAL TABLE ext_expenses    LOCATION ('gphdfs://hdfshost-1:8081/data/?compress=true&compression_type=RECORD   &codec=org.apache.hadoop.io.compress.DefaultCodec')    FORMAT 'custom' (formatter='gphdfs_export');
To read custom-formatted data:* Create and run a MapReduce job that creates a copy of the data in a format accessible to Greenplum Database.* Use CREATE EXTERNAL TABLE to read the data into Greenplum Database.Geenplum provides Java APIs for use in the MapReduce code. The Javadoc is available in the $GPHOME/docs directory.Example readable custom-formatted external table: https://gpdb.docs.pivotal.io/43160/admin_guide/load/topics/g-sample-mapreduce-code.htmlExample writable custom-formatted external table: https://gpdb.docs.pivotal.io/43160/admin_guide/load/topics/g-mapreduce-sample-code.html
When Greenplum Database accesses external table data from an HDFS location with gphdfs protocol, each Greenplum Database segment on a host system starts a JVM for use by the protocol. The default JVM heapsize is 1GB and should be enough for most workloads. If needed you can increase the JVM heapsize by changing GP_JAVA_OPT variable in the file $GPHOME/lib/hadoop/hadoop_env.sh.
export GP_JAVA_OPT='-Xmx1000m -XX:+DisplayVMOutputToStderr'The $GPHOME/lib/hadoop/hadoop_env.sh must be updated for every segment instance in the Greenplum Database system. ncreasing the Java -Xmx value from 1GB to 2GB results in 16GB allocated in an environment of 8 segments per host.
=== Avro 
An Avro file stores both the data definition (schema) and the data together in one file making it easy for programs to dynamically understand the information stored in an Avro file. The Avro schema is in JSON format, the data is in a binary format making it compact and efficient.
The following example Avro schema defines an Avro record with 3 fields: (name, favorite_number, favorite_color) {"namespace": "example.avro", "type": "record", "name": "User",  "fields": [    {"name": "name", "type": "string"},         {"name": "favorite_number", "type": ["int", "null"]},    {"name": "favorite_color", "type": ["string", "null"]}  ]}These are two rows of data based on the schema:{ "name" : "miguno" , "favorite_number" : 6 , "favorite_color" : "red" }{ "name" : "BlizzardCS" , "favorite_number" : 21 , "favorite_color" : "green" }
Simple CREATE EXTERNAL TABLE command that reads data from the two Avro fields id and ba. CREATE EXTERNAL TABLE avro1 (id int, ba bytea[])    LOCATION ('gphdfs://my_hdfs:8020/avro/singleAvro/array2.avro')    FORMAT 'avro';
Specifiey overwrite Avro schema that is the gphdfs protocol uses to create the Avro file.CREATE WRITABLE EXTERNAL TABLE at1w(id int, names text[], nums int[])    LOCATION ('gphdfs://my_hdfs:8020/tmp/at1      ?schema=hdfs://my_hdfs:8020/avro/array_simple.avsc')   FORMAT 'avro';
Write to an Avro file and specifies a namespace for the Avro schema.CREATE WRITABLE EXTERNAL TABLE atudt1 (id int, info myt, birth date, salary numeric )    LOCATION ('gphdfs://my_hdfs:8020/tmp/emp01.avro      ?namespace=public.pivotal.avro')    FORMAT 'avro';
=== Parquet 
Parquet allows compression schemes to be specified on a per-column level, and supports adding more encodings as they are invented and implemented.
 In a Parquet file, the metadata (Parquet schema definition) contains data structure information is written after the data to allow for single pass writing. Example:
message test {    repeated byte_array binary_field;    required int32 int32_field;    optional int64 int64_field;    required boolean boolean_field;    required fixed_len_byte_array(3) flba_field;    required byte_array someDay (utf8);    };
Example external table definition for a Parquet file.CREATE WRITABLE EXTERNAL TABLE films (   code char(5),    title varchar(40),   id integer,   date_prod date,    subtitle boolean) LOCATION ( 'gphdfs://my-films') FORMAT 'PARQUET' ;
== External Web Tables 
=== EXECUTE 
By default the script runs on segments and executes as many instances as number of primary segments on the segment host. You can optionally limit the number of segment instances that execute the web table command. All segments included in the web table definition in the ON clause run the command in parallel.
The command that you specify in the external table definition executes from the database and cannot access environment variables from .bashrc or .profile. Set environment variables in the EXECUTE clause. For example:
CREATE EXTERNAL WEB TABLE output (output text)    EXECUTE 'PATH=/home/gpadmin/programs; export PATH; myprogram.sh'     FORMAT 'TEXT';
ExampleCREATE EXTERNAL WEB TABLE log_output     (linenum int, message text)     EXECUTE '/var/load_scripts/get_log_data.sh' ON HOST     FORMAT 'TEXT' (DELIMITER '|');
Writable External Web Table with ScriptCREATE WRITABLE EXTERNAL WEB TABLE campaign_out    (LIKE campaign)    EXECUTE '/var/unload_scripts/to_adreport_etl.sh'    FORMAT 'TEXT' (DELIMITER '|');
?? verify is it really runs as many instances as segments 
== Custom non-HDFS format 
You specify a custom data format in the FORMAT clause of CREATE EXTERNAL TABLE.
FORMAT 'CUSTOM' (formatter=format_function, key1=val1,...keyn=valn)Where the 'CUSTOM' keyword indicates that the data has a custom format and formatter specifies the function to use to format the data, followed by comma-separated parameters to the formatter function.
Greenplum Database provides functions for formatting fixed-width data, but you must author the formatter functions for variable-width data. The steps are as follows.* Author and compile input and output functions as a shared library.* Specify the shared library function with CREATE FUNCTION in Greenplum Database.* Use the formatter parameter of CREATE EXTERNAL TABLE's FORMAT clause to call the function.
Load fixed with data with the functions fixedwith_in and fixedwidth_out. These functions already exist in the file $GPHOME/share/postgresql/cdb_external_extensions.sql
CREATE READABLE EXTERNAL TABLE students (name varchar(20), address varchar(30), age int)LOCATION ('file://<host>/file/path/')FORMAT 'CUSTOM' (formatter=fixedwidth_in,          name='20', address='30', age='4');
Load training blanks as NULLsCREATE READABLE EXTERNAL TABLE students (name varchar(20), address varchar(30), age int)LOCATION ('gpfdist://<host>:<portNum>/file/path/')FORMAT 'CUSTOM' (formatter=fixedwidth_in,          name=20, address=30, age=4,        preserve_blanks='on',null='NULL');
Custom line delimiter CREATE WRITABLE EXTERNAL TABLE students_out (name varchar(20), address varchar(30), age int)LOCATION ('gpfdist://<host>:<portNum>/file/path/students_out.txt')     FORMAT 'CUSTOM' (formatter=fixedwidth_out,         name=20, address=30, age=4, line_delim=E'\r\n');
=== Creating custom protocol 
If you need to connect to another system directly and stream data from one to the other.* Author the send, receive, and (optionally) validator functions in C, with a predefined API. 
After writing and compiling the read and write functions into a shared object (.so), declare a database function that points to the .so file and function names.
CREATE FUNCTION myread() RETURNS integeras '$libdir/gpextprotocol.so', 'myprot_import'LANGUAGE C STABLE;
CREATE FUNCTION mywrite() RETURNS integeras '$libdir/gpextprotocol.so', 'myprot_export'LANGUAGE C STABLE;
Create a protocol that accesses these functions. Validatorfunc is optional.CREATE TRUSTED PROTOCOL myprot(writefunc='mywrite',readfunc='myread');
Use the protocol in readable or writable external tables.CREATE READABLE EXTERNAL TABLE ext_sales(LIKE sales)LOCATION('myprot://<meta>/<meta>/…')FORMAT 'TEXT';
Interface and implementation example: https://gpdb.docs.pivotal.io/43160/admin_guide/load/topics/g-example-custom-data-access-protocol.html#topic103
== Error handling 
Access to the external table in "single row error isolation mode" enabled by specifying SEGMENT REJECT LIMIT clause.
CREATE EXTERNAL TABLE ext_expenses ( name text,    date date, amount float4, category text, desc1 text )    LOCATION ('gpfdist://etlhost-1:8081/*.txt',              'gpfdist://etlhost-2:8082/*.txt')   FORMAT 'TEXT' ( DELIMITER '|' NULL ' ')   LOG ERRORS SEGMENT REJECT LIMIT 5;
SEGMENT REJECT LIMIT is per-segment, not per entire operation (i.e. 10 rows per segment can be rejected)
gp_initial_bad_row_limit limits the initial number of rows that are processed that are not formatted properly. The default is to stop processing if the first 1000 rows contain formatting errors. 
Log errors internally and set an error threshold of 10 errors.# CREATE EXTERNAL TABLE ext_expenses ( name text,    date date,  amount float4, category text, desc1 text )    LOCATION ('gpfdist://etlhost-1:8081/*',              'gpfdist://etlhost-2:8082/*')   FORMAT 'TEXT' (DELIMITER '|')   LOG ERRORS SEGMENT REJECT LIMIT 10      ROWS;
Use the built-in SQL function gp_read_error_log('external_table') to read the error log data. This example command displays the log errors for ext_expenses (format: https://gpdb.docs.pivotal.io/43160/admin_guide/load/topics/g-viewing-bad-rows-in-the-error-table-or-error-log.html#topic58)
SELECT gp_read_error_log('ext_expenses');
Remove log error records for a table SELECT gp_truncate_error_log('ext_expenses'); 
Log into specific error table. Table is being created on the flyCREATE EXTERNAL TABLE ext_expenses ( name text,    date date,  amount float4, category text, desc1 text )    LOCATION ('gpfdist://etlhost-1:8081/*',              'gpfdist://etlhost-2:8082/*')   FORMAT 'TEXT' (DELIMITER '|')   LOG ERRORS INTO err_expenses SEGMENT REJECT LIMIT 10      ROWS;
Storing row formatting in a error table is deprecated in Greenplum Database and will not be supported in a the next major release. Only internal error logs will be supported
If a CSV file contains invalid formatting, the rawdata field in the error table can contain several combined rows. For example, if a closing quote for a specific field is missing, all the following newlines are treated as embedded newlines. When this happens, Greenplum stops parsing a row when it reaches 64K, puts that 64K of data into the error table as a single row, resets the quote flag, and continues. If this happens three times during load processing, the load file is considered invalid and the entire load fails with the message "rejected N or more rows". 
Run VACUUM after load errors. If the load operation does not run in single row error isolation mode, the operation stops at the first error. The target table contains the rows loaded before the error occurred.
= gpload 
YAML example
---VERSION: 1.0.0.1DATABASE: opsUSER: gpadminHOST: mdw-1PORT: 5432GPLOAD:   INPUT:    - SOURCE:         LOCAL_HOSTNAME:           - etl1-1           - etl1-2           - etl1-3           - etl1-4         PORT: 8081         FILE:            - /var/load/data/*    - COLUMNS:           - name: text- amount: float4           - category: text           - desc: text           - date: date    - FORMAT: text    - DELIMITER: '|'    - ERROR_LIMIT: 25    - ERROR_TABLE: payables.err_expenses   OUTPUT:    - TABLE: payables.expenses    - MODE: INSERTSQL:   - BEFORE: "INSERT INTO audit VALUES('start', current_timestamp)"   - AFTER: "INSERT INTO audit VALUES('end', current_timestamp)"
Run gpload, passing in the load control file.
gpload -f my_load.yml

----------------
= Copy 
COPY FROM copies data from a file *or* standard input into a table and appends the data to the table contents. 
The COPY source file must be accessible to the master host. Greenplum copies data from STDIN or STDOUT using the connection between the client and the master server.
COPY can be also excuted in single row error isolation mode which applies only to rows in the input file that contain format errors. If the data contains a contraint error such as violation of a NOT NULL, CHECK, or UNIQUE constraint, the operation fails and no data loads.
Log errors clause logs errors internally 
=> COPY country FROM '/data/gpdb/country_data'    WITH DELIMITER '|' LOG ERRORS   SEGMENT REJECT LIMIT 10 ROWS;

------------------
= indexes 
You can temporarily increase the maintenance_work_mem server configuration parameter to help speed up CREATE INDEX commands, though load performance is affected. 
